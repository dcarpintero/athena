title = "Athena Prompt Templates for Cohere's API"

[abstract]
prompt =    """
            You are a proficient assistant in Natural Language Processing (NLP). You will be given the abstract of a research paper. 
            Your task is to enrich technical Named Entities with Wikipedia links as outlined in the Enriched_Text section of the EXAMPLE.
            Focus on Named Entities that are relevant to the fields of Artificial Intelligence (AI), Machine Learning (ML), Algorithms, Natural Language Processing, and Computer Science. 
            This enrichment aims to provide comprehensive, linked contextual information for each technical term, enhancing the reader's understanding and access to further resources.
            Make sure the links that you include belong to the wikipedia.org domain, and provide your reponse in markdown format as in the example:

            EXAMPLE:
            Text: 
            We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. 
            Unlike recent language representation models, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly 
            conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional
            output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial
            task specific architecture modifications.

            Response:
            We introduce a new language representation model called [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)), 
            which stands for [Bidirectional Encoder Representations from Transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)). 
            Unlike recent language representation models, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly 
            conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional 
            output layer to create state-of-the-art models for a wide range of tasks, such as [question answering](https://en.wikipedia.org/wiki/Question_answering) 
            and [language inference](https://en.wikipedia.org/wiki/Natural_language_inference), without substantial task-specific architecture modifications.
            --

            TASK:
            Text: 
            {text}
            """

[tweet]
prompt =    """
            Create a JSON-formatted response for a Tweet about a research paper.
            The research paper is about: '{summary}' and the link is: '{link}'.
            A well-formed Tweet includes three elements: (1) text that sparks curiosity, (2) this link {link} to the paper, and (3) hashtags.
    
            EXAMPLE:
            {{
                "text" : "Exploring AI's language frontiers with 'BERT: Pre-training of Deep Bidirectional Transformers' by Devlin et al. (2018).
                          BERT revolutionizes NLP with state-of-the-art results across various tasks.
                          Read more: https://arxiv.org/abs/1810.04805 #AI #NLP #MachineLearning ðŸ¤–ðŸ’¬",
            }} 
            
            ---
            """

[email]
prompt =    """
            Create a JSON-formatted response for a professional cold email. The email is from myself, {sender}, a researcher at {institution}, \
            addressed to {receivers}, authors of the research paper '{title}'. The email should express respect for their work, \
            briefly introduce my research interests, and inquire about their willingness to collaborate on an upcoming project that aligns with our mutual interests.

            EXAMPLE:
            {{
                "subject" : "Collaboration on {topic} with {institution}"
                "body" : "Dear {receivers},\n\nI hope this message finds you well. My name is {sender}, and I'm a researcher specializing in {topic} at {institution}. \ 
                          After reading your influential paper, '{topic}', I was deeply impressed by your insights and findings. And I am reaching out to explore the \
                          possibility of collaborating on a project that I believe could benefit greatly from your expertise. I would be honored to discuss this further \
                          if you are interested.\n\nLooking forward to the possibility of working together.\n\nBest regards,\n{sender}"
            }}

            ---
            """

[keywords]
prompt =    """
            You are a helpful research assistant. Your task is to analyze the abstract of a research paper and to identify the most relevant keywords that 
            capture the core concepts and technologies discussed. For each keyword, provide a brief explanation of its significance in the context of this research. 
            Focus on terms related to the paper's primary field, including any specific methodologies, technologies, theories, or applications mentioned. 
            Your explanations should help in understanding the abstract's main contributions and the implications of the research in its respective field.

            {text}
            """